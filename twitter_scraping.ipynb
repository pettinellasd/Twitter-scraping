{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a56ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements install this first, before running the script\n",
    "# pip install langdetect\n",
    "# pip install nltk\n",
    "# import nltk\n",
    "# nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter # The libraby used to scrape\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # library used to do sentiment analysis\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# Creating list to append tweet data \n",
    "for i in range (1,30): # date range from 1-30\n",
    "    print(\"Scraping tweets of date\", i)\n",
    "    filename = \"twitter/twitter\"+str(i)+\".csv\"\n",
    "    tweets_list2 = []\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('bitcoin since:2022-07-'+str(i)+ ' until:2022-07-'+str(i+1)).get_items()):\n",
    "        try:  \n",
    "            #Perform sentiment analysis on the tweet\n",
    "            title_score = vader.polarity_scores(str(tweet.content))\n",
    "            # Create a list with the tween content, sentiment score compound and date\n",
    "            tweets_list2.append([tweet.content, title_score['compound'],tweet.date])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df2 = pd.DataFrame(tweets_list2, columns=['Text', 'Score','Datetime'])\n",
    "    tweets_df2.to_csv(filename)\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "811eee06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter/twitter1.csv\n",
      "twitter/twitter2.csv\n",
      "twitter/twitter3.csv\n",
      "twitter/twitter4.csv\n",
      "twitter/twitter5.csv\n",
      "twitter/twitter6.csv\n",
      "twitter/twitter7.csv\n",
      "twitter/twitter8.csv\n",
      "twitter/twitter9.csv\n",
      "twitter/twitter10.csv\n",
      "twitter/twitter11.csv\n",
      "twitter/twitter12.csv\n",
      "twitter/twitter13.csv\n",
      "twitter/twitter14.csv\n",
      "twitter/twitter15.csv\n",
      "twitter/twitter16.csv\n",
      "twitter/twitter17.csv\n",
      "twitter/twitter18.csv\n",
      "twitter/twitter19.csv\n",
      "twitter/twitter20.csv\n",
      "twitter/twitter21.csv\n",
      "twitter/twitter22.csv\n",
      "twitter/twitter23.csv\n",
      "twitter/twitter24.csv\n",
      "twitter/twitter25.csv\n",
      "twitter/twitter26.csv\n",
      "twitter/twitter27.csv\n",
      "twitter/twitter28.csv\n",
      "twitter/twitter29.csv\n",
      "twitter/twitter30.csv\n",
      "twitter/twitter31.csv\n",
      "Created new csv file, with all the twitter tweets from each day file\n"
     ]
    }
   ],
   "source": [
    "#PROCESSING TWEETS FOR ALL DIFFERENT CSV FILES 1-30 DATE\n",
    "import csv\n",
    "#The below list will store all the tweets from each day\n",
    "all_data =[]\n",
    "j = 0\n",
    "i = 0\n",
    "while True:\n",
    "    i = i+1\n",
    "    #Close loop if last file is reached\n",
    "    if i == 32:\n",
    "        break\n",
    "    with open('twitter/twitter'+str(i)+'.csv',encoding='UTF8', newline='') as f:\n",
    "        print('twitter/twitter'+str(i)+'.csv')\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) #Skip header\n",
    "        for row in reader:\n",
    "            j = j+1\n",
    "            try:\n",
    "                ts = [j ,row[1].replace(\";\",\".\"), row[2], row[3]]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            all_data.append(ts)\n",
    "            \n",
    "# Write the new combined list to a new csv file.            \n",
    "with open(\"twitter_scrape.csv\", 'w',newline ='', encoding='UTF8') as file: \n",
    "    write = csv.writer(file)\n",
    "    header = [\"id\", \"tweet\", \"Sentiment Compound\", \"date\"]\n",
    "    write.writerow(header)\n",
    "    write.writerows(all_data)\n",
    "print(\"Created new csv file, with all the twitter tweets from each day file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO seperate english vs non english tweets . This will take a lot of time.\n",
    "import csv\n",
    "from langdetect import detect\n",
    "en = 0\n",
    "nen = 0\n",
    "en_dict = [] #Stores the list of english tweets\n",
    "nen_dict = [] # Stores list of non english tweets\n",
    "with open('twitter_scrape.csv',encoding='UTF8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        try:\n",
    "            s = detect(row[1])\n",
    "            if s != \"en\":\n",
    "                nen_dict.append(row)\n",
    "                #print(row[1])\n",
    "            else:\n",
    "                en_dict.append(row)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b92152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects the index for the final csv\n",
    "i = 0\n",
    "new_en_dict = []\n",
    "for en in en_dict:\n",
    "    i = i+1\n",
    "    en[0] = i\n",
    "    new_en_dict.append(en)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8734160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes the final list to the csv file, which has all the english tweets\n",
    "with open(\"twitter_scraping_non-english.csv\", 'w',newline ='', encoding='UTF8') as file: \n",
    "    write = csv.writer(file)\n",
    "    header = [\"id\", \"comments\", \"Sentiment Compound\", \"date\"]\n",
    "    write.writerow(header)\n",
    "    write.writerows(new_en_dict)\n",
    "print(\"Created new csv file, with all the twitter tweets from each day file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
